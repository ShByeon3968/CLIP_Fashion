from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch
import os
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import SystemMessage, HumanMessage
from transformers import CLIPTokenizer
from typing import List

# OpenAI API Key ì„¤ì •


# 1. BLIP ì„¤ì • (ì´ë¯¸ì§€ â†’ ì„¤ëª…)
device = "cuda" if torch.cuda.is_available() else "cpu"
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def generate_caption(image: Image.Image) -> str:
    """
    ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„± (BLIP ê¸°ë°˜)
    """
    inputs = blip_processor(image, return_tensors="pt").to(device)
    out = blip_model.generate(**inputs, max_new_tokens=30)
    caption = blip_processor.decode(out[0], skip_special_tokens=True)
    return caption


# 2. LangChain ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def suggest_prompt_langchain(caption: str) -> str:
    """
    LangChain PromptTemplateì„ ì‚¬ìš©í•œ GPT-3.5 ê¸°ë°˜ ë¦¬ë””ìì¸ ëª…ë ¹ ìƒì„±
    """
    # LangChain LLM ì„¤ì •
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=50
    )

    # í…œí”Œë¦¿ ì •ì˜
    prompt_template = PromptTemplate(
        input_variables=["caption"],
        template=(
            "ì´ ì„¤ëª…ì€ ì˜· ì´ë¯¸ì§€ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤:\n\"{caption}\"\n\n"
            "ì‚¬ìš©ìê°€ ì´ ì˜·ì„ ì–´ë–»ê²Œ ë¦¬ë””ìì¸í•˜ë©´ ì¢‹ì„ì§€ í•œêµ­ì–´ë¡œ 1ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•´ ì£¼ì„¸ìš”. "
            "ì˜ˆì‹œ: \"please change this cloth to red color\", \"please remove the logo\"\n\n"
            "ì¶”ì²œ:"
        )
    )

    prompt = prompt_template.format(caption=caption)

    try:
        response = llm([
            SystemMessage(content="ë‹¹ì‹ ì€ íŒ¨ì…˜ ë¦¬ë””ìì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ])
        return response.content.strip()
    except Exception as e:
        print("[LangChain GPT Error]", e)
        return "ì´ ì˜·ì„ ë” ìŠ¤íƒ€ì¼ë¦¬ì‹œí•˜ê²Œ ë°”ê¿”ì¤˜"
    
def translate_to_english(korean_prompt: str) -> str:
    """
    í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ê³ í’ˆì§ˆ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    (Stable Diffusionìš© í”„ë¦¬ì…‹ ìŠ¤íƒ€ì¼ êµ¬ì„±: ì˜ë¥˜ ë³€ê²½ ë‚´ìš© + ìŠ¤íƒ€ì¼ + í’ˆì§ˆ + êµ¬ë„)
    """
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.5,
        max_tokens=200
    )

    try:
        response = llm([
            SystemMessage(
                content=(
                    "You are a professional prompt engineer for Stable Diffusion XL. "
                    "Your job is to convert Korean fashion redesign instructions into high-quality English prompts "
                    "suitable for text-to-image generation.\n\n"
                    "ğŸ’¡ Important Instructions:\n"
                    "- Do NOT write full sentences.\n"
                    "- Output should be a list of descriptive **keywords**.\n"
                    "- Use a **preset-style structure** combining:\n"
                    "   Clothing description (translated and redesigned),Style type (e.g., high fashion, editorial photo)"
                )
            ),
            HumanMessage(
                content=f'Korean Instruction: "{korean_prompt}"\n\nPlease return the full prompt below:'
            )
        ])
        return response.content.strip()
    except Exception as e:
        print("[Prompt Translation Error]", e)
        return "blue sleeveless hoodie, high fashion, full body, 8k, soft lighting, studio background"

def truncate_prompt(prompt: str, max_tokens: int = 77) -> str:
    tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
    tokens = tokenizer.tokenize(prompt)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
    return tokenizer.convert_tokens_to_string(tokens)

def improve_prompt_with_llm(bad_prompt: str, visual_goals: List[str] = None) -> str:
    """
    ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ëª…í™•í•˜ê²Œ ì‹œê°ì ìœ¼ë¡œ ë³´ì™„í•˜ëŠ” í•¨ìˆ˜
    - bad_prompt: ì›ë˜ì˜ ë¶€ì •í™•í•˜ê±°ë‚˜ ëª¨í˜¸í•œ í”„ë¡¬í”„íŠ¸
    - visual_goals: í”„ë¡¬í”„íŠ¸ì— ë°˜ë“œì‹œ ë°˜ì˜ë˜ì–´ì•¼ í•  ì‹œê°ì  ëª©í‘œë“¤
    """
    if visual_goals is None:
        visual_goals = [
            "Place a large white logo at the center of the chest",
            "Use a red sweatshirt as the base garment",
            "Ensure the logo is bold and clearly visible",
        ]

    goal_text = "\n".join(f"- {g}" for g in visual_goals)

    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.5)

    messages = [
        SystemMessage(content="You are a professional prompt engineer for Stable Diffusion. Your job is to rewrite prompts to be more visually descriptive and specific."),
        HumanMessage(content=f"""
Original Prompt:
\"{bad_prompt}\"

Visual Goals to Apply:
{goal_text}

Please rewrite the prompt in a visually descriptive and effective format for Stable Diffusion. Keep it concise but specific.
""")
    ]

    improved_prompt = llm(messages).content.strip()
    return improved_prompt