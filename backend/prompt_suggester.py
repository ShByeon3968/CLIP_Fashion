from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch
import os
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import SystemMessage, HumanMessage

# OpenAI API Key ì„¤ì •


# 1. BLIP ì„¤ì • (ì´ë¯¸ì§€ â†’ ì„¤ëª…)
device = "cuda" if torch.cuda.is_available() else "cpu"
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

def generate_caption(image: Image.Image) -> str:
    """
    ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„± (BLIP ê¸°ë°˜)
    """
    inputs = blip_processor(image, return_tensors="pt").to(device)
    out = blip_model.generate(**inputs, max_new_tokens=30)
    caption = blip_processor.decode(out[0], skip_special_tokens=True)
    return caption


# 2. LangChain ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def suggest_prompt_langchain(caption: str) -> str:
    """
    LangChain PromptTemplateì„ ì‚¬ìš©í•œ GPT-3.5 ê¸°ë°˜ ë¦¬ë””ìì¸ ëª…ë ¹ ìƒì„±
    """
    # LangChain LLM ì„¤ì •
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.7,
        max_tokens=50
    )

    # í…œí”Œë¦¿ ì •ì˜
    prompt_template = PromptTemplate(
        input_variables=["caption"],
        template=(
            "ì´ ì„¤ëª…ì€ ì˜· ì´ë¯¸ì§€ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤:\n\"{caption}\"\n\n"
            "ì‚¬ìš©ìê°€ ì´ ì˜·ì„ ì–´ë–»ê²Œ ë¦¬ë””ìì¸í•˜ë©´ ì¢‹ì„ì§€ í•œêµ­ì–´ë¡œ 1ë¬¸ì¥ìœ¼ë¡œ ì¶”ì²œí•´ ì£¼ì„¸ìš”. "
            "ì˜ˆì‹œ: \"please change this cloth to red color\", \"please remove the logo\"\n\n"
            "ì¶”ì²œ:"
        )
    )

    prompt = prompt_template.format(caption=caption)

    try:
        response = llm([
            SystemMessage(content="ë‹¹ì‹ ì€ íŒ¨ì…˜ ë¦¬ë””ìì¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ])
        return response.content.strip()
    except Exception as e:
        print("[LangChain GPT Error]", e)
        return "ì´ ì˜·ì„ ë” ìŠ¤íƒ€ì¼ë¦¬ì‹œí•˜ê²Œ ë°”ê¿”ì¤˜"
    
def translate_to_english(korean_prompt: str) -> str:
    """
    í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ê³ í’ˆì§ˆ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    (Stable Diffusionìš© í”„ë¦¬ì…‹ ìŠ¤íƒ€ì¼ êµ¬ì„±: ì˜ë¥˜ ë³€ê²½ ë‚´ìš© + ìŠ¤íƒ€ì¼ + í’ˆì§ˆ + êµ¬ë„)
    """
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.5,
        max_tokens=200
    )

    try:
        response = llm([
            SystemMessage(
                content=(
                    "You are a professional prompt engineer for Stable Diffusion XL. "
                    "Your job is to convert Korean fashion redesign instructions into high-quality English prompts "
                    "suitable for text-to-image generation.\n\n"
                    "ğŸ’¡ Important Instructions:\n"
                    "- Do NOT write full sentences.\n"
                    "- Output should be a list of descriptive **keywords**.\n"
                    "- Use a **preset-style structure** combining:\n"
                    "   1. Clothing description (translated and redesigned)\n"
                    "   2. Style type (e.g., high fashion, editorial photo)\n"
                    "   3. Viewpoint (e.g., full body, front view)\n"
                    "   4. Lighting/quality (e.g., soft lighting, 8k, studio background)\n\n"
                    "ğŸ¯ Final Output Format Example:\n"
                    "   blue sleeveless hoodie, high fashion, full body, 8k, soft lighting, studio background"
                )
            ),
            HumanMessage(
                content=f'Korean Instruction: "{korean_prompt}"\n\nPlease return the full prompt below:'
            )
        ])
        return response.content.strip()
    except Exception as e:
        print("[Prompt Translation Error]", e)
        return "blue sleeveless hoodie, high fashion, full body, 8k, soft lighting, studio background"
    